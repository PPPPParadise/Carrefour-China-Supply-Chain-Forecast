{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Planning KPI Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, datetime, time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib. dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from impala.dbapi import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters.\n",
      "Detention:\t20190717\t20190915\n",
      "Stock level:\t20190717\t20190915\n",
      "Consistency:\t20190717\t20190915\n",
      "Service level:\t20190717\t20190915\n",
      "OOS_CHECK_DATE:\t20190915\n"
     ]
    }
   ],
   "source": [
    "## Get parameters, if not given then fall back to default values\n",
    "\n",
    "tfmt = '%Y%m%d'\n",
    "_end = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime(tfmt)\n",
    "\n",
    "if 'MONITOR_RUN_DATE' in os.environ:\n",
    "    print('Using external parameters.')\n",
    "    _end = os.environ.get('MONITOR_RUN_DATE')\n",
    "else:\n",
    "    print('Using default parameters.')\n",
    "    \n",
    "_start = (datetime.datetime.strptime(_end, '%Y%m%d').date() - datetime.timedelta(days=60)).strftime(tfmt)\n",
    "date_str = _end\n",
    "\n",
    "DETENTION_START, DETENTION_END = _start, _end\n",
    "SERVICE_LEVEL_START, SERVICE_LEVEL_END = _start, _end\n",
    "STOCK_LEVEL_START, STOCK_LEVEL_END = _start, _end\n",
    "CONSISTENCY_START, CONSISTENCY_END = _start, _end\n",
    "OOS_CHECK_DATE = _end\n",
    "\n",
    "print('Detention:', DETENTION_START, DETENTION_END, sep='\\t')\n",
    "print('Stock level:', STOCK_LEVEL_START, STOCK_LEVEL_END, sep='\\t')\n",
    "print('Consistency:', CONSISTENCY_START, CONSISTENCY_END, sep='\\t')\n",
    "print('Service level:', SERVICE_LEVEL_START, SERVICE_LEVEL_END, sep='\\t')\n",
    "print('OOS_CHECK_DATE:', OOS_CHECK_DATE, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_folder = '/data/jupyter/Carrefour-China-Supply-Chain-Forecast/output/monitoring/'\n",
    "\n",
    "detention_rate_dc_file = f\"report_detention_rate_dc_{date_str}.xlsx\"\n",
    "\n",
    "detention_rate_store_file = f\"report_detention_rate_store_{date_str}.xlsx\"\n",
    "\n",
    "stock_level_dc_file = f\"report_stock_level_dc_{date_str}.xlsx\"\n",
    "\n",
    "stock_level_store_file = f\"report_stock_level_store_{date_str}.xlsx\"\n",
    "\n",
    "service_level_file = f\"report_service_level_{date_str}.xlsx\"\n",
    "\n",
    "consistency_file = f'report_consistency_items_{date_str}.xlsx'\n",
    "\n",
    "oos_file = f'report_oos_item_list_{date_str}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generation time: 2019-09-16 18:17:06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Report generation time:', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), end='\\n\\n')\n",
    "T0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars /data/jupyter/kudu-spark2_2.11-1.8.0.jar pyspark-shell'\n",
    "warehouse_location = os.path.abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Forecast monitoring process\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.blacklist.enabled\", False) \\\n",
    "    .config(\"spark.driver.memory\", '6g') \\\n",
    "    .config(\"spark.executor.memory\", '6g') \\\n",
    "    .config(\"spark.num.executors\", '14') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudu_tables = [\n",
    "    'lfms.daily_dctrxn', 'lfms.daily_dcstock', 'lfms.ord', 'lfms.daily_shipment'\n",
    "]\n",
    "\n",
    "for tbl in kudu_tables:\n",
    "    spark.read.format('org.apache.kudu.spark.kudu') \\\n",
    "    .option('kudu.master', \"dtla1apps11:7051,dtla1apps12:7051,dtla1apps13:7051\") \\\n",
    "    .option('kudu.table', f'impala::{tbl}') \\\n",
    "    .load() \\\n",
    "    .registerTempTable('{}'.format(tbl.replace('.', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(sql_path, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "   \n",
    "    query = query.format(**query_params)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query_and_fetch(sql_path, create_table=False, get_query=False, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "    if not create_table:\n",
    "        ## remove lines with `table`\n",
    "        q0 = query\n",
    "        query = '\\n'.join([line for line in q0.split('\\n')\n",
    "                           if ('drop table' not in line.lower())\n",
    "                           and ('create table' not in line.lower())])\n",
    "    query = query.format(**query_params)\n",
    "    if get_query:\n",
    "        return query\n",
    "    return spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_with_impala(sql):\n",
    "    with connect(host='dtla1apps14', port=21050, auth_mechanism='PLAIN', user='CHEXT10211', password='datalake2019',\n",
    "                 database='vartefact') as conn:\n",
    "        curr = conn.cursor()\n",
    "        curr.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_stock_sql = get_query(\n",
    "    'sql/record_dc_stock.sql',\n",
    "    database_name='vartefact', date_start=DETENTION_START, date_end=DETENTION_END\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql_with_impala(dc_stock_sql.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_stock_sql = get_query(\n",
    "    'sql/record_store_stock.sql',\n",
    "    database_name='vartefact', date_start=DETENTION_START, date_end=DETENTION_END\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql_with_impala(store_stock_sql.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detention rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Detention rate - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_dc.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_supplier = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_dc_supplier.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_writer = ExcelWriter(record_folder + detention_rate_dc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_dc(df, title):\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    flow_A = df1[df1.rotation == 'A']\n",
    "    flow_B = df1[df1.rotation == 'B']\n",
    "    ax.plot(flow_A.date_dt, flow_A.detention_rate, label='Flow A')\n",
    "    ax.plot(flow_B.date_dt, flow_B.detention_rate, label='Flow B')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'DC detention rate for {title}')\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    print(f'Latest detention rate for {title}:')\n",
    "    display(df[['date_key', 'rotation', 'detention_rate']].tail(9).style.hide_index())\n",
    "    \n",
    "    df1.to_excel(detention_dc_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc, 'all items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_writer.save()\n",
    "print(f'Please check file {detention_rate_dc_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Detention rate - Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_all_store = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_all_store.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_store.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store_writer = ExcelWriter(record_folder + detention_rate_store_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_all_store(df):\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    flow_A = df1[df1.rotation == 'A']\n",
    "    flow_B = df1[df1.rotation == 'B']\n",
    "    flow_X = df1[df1.rotation == 'X']\n",
    "    ax.plot(flow_A.date_dt, flow_A.detention_rate, label='Flow A')\n",
    "    ax.plot(flow_B.date_dt, flow_B.detention_rate, label='Flow B')\n",
    "    ax.plot(flow_X.date_dt, flow_X.detention_rate, label='Flow X')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Store detention rate by day')\n",
    "    fig.autofmt_xdate()\n",
    "    print(f'Latest detention rate:')\n",
    "\n",
    "    display(df[['date_key', 'rotation', 'detention_rate']].tail(9).style.hide_index())\n",
    "    \n",
    "    flow_A.to_excel(detention_store_writer, sheet_name=\"Flow A\", index=False)\n",
    "    flow_B.to_excel(detention_store_writer, sheet_name=\"Flow B\", index=False)\n",
    "    flow_X.to_excel(detention_store_writer, sheet_name=\"Flow X\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_store(df):\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key', 'store_code']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "\n",
    "    dr = df1.groupby(['store_code', 'rotation'])['detention_rate'].mean().reset_index()\n",
    "    a = dr.loc[dr.rotation == 'A', 'detention_rate'].sort_values().values\n",
    "    b = dr.loc[dr.rotation == 'B', 'detention_rate'].sort_values().values\n",
    "    x = dr.loc[dr.rotation == 'X', 'detention_rate'].sort_values().values\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(16, 4), ncols=3)\n",
    "\n",
    "    axes[0].plot(a, 'o', ms=6)\n",
    "    axes[1].plot(b, 'o', ms=6)\n",
    "    axes[2].plot(x, 'o', ms=6)\n",
    "    \n",
    "    axes[0].set_title('Store detention rate by store: Flow A')\n",
    "    axes[1].set_title('Store detention rate by store: Flow B')\n",
    "    axes[2].set_title('Store detention rate by store: Flow X')\n",
    "    \n",
    "    axes[0].set_ylim(min(0.75, a.min()), 1)\n",
    "    axes[1].set_ylim(min(0.75, b.min()), 1)\n",
    "    axes[2].set_ylim(min(0.75, x.min()), 1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    df1[df1.rotation == 'A'].to_excel(detention_store_writer, sheet_name=\"By store flow A\", index=False)\n",
    "    df1[df1.rotation == 'B'].to_excel(detention_store_writer, sheet_name=\"By store flow B\", index=False)\n",
    "    df1[df1.rotation == 'X'].to_excel(detention_store_writer, sheet_name=\"By store flow X\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_all_store(detention_all_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_store(detention_store) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store_writer.save()\n",
    "print(f'Please check file {detention_rate_store_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Stock level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Stock level - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_level_dc = spark.sql(get_query(\n",
    "    'sql/kpi_stock_level_dc.sql',\n",
    "    # database_name='vartefact', date_start='20190630', date_end='20190730',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dc_writer = ExcelWriter(record_folder + stock_level_dc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_level_dc(df, title):\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    sl = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sl['date_key'] = pd.to_datetime(sl.date_key, format='%Y%m%d')\n",
    "    fig, axes = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "    for i, rotation in enumerate(['A', 'B']):\n",
    "        axes[i].set_title(f'DC stock level {title} Rotation {rotation}')\n",
    "        for dm in sl.in_dm.unique():\n",
    "            d = sl[(sl.in_dm == dm) & (sl.rotation == rotation)]\n",
    "            axes[i].plot(d.date_key, d.stock_level, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes[i].legend()\n",
    "    fig.autofmt_xdate(); fig.tight_layout()\n",
    "    \n",
    "    df1.to_excel(stock_dc_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc, 'all items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dc_writer.save()\n",
    "print(f'Please check file {stock_level_dc_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Stock level - store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_level_store = spark.sql(get_query(\n",
    "    'sql/kpi_stock_level_store.sql',\n",
    "    # database_name='vartefact', date_start='20190630', date_end='20190730',\n",
    "    database_name='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_store_writer = ExcelWriter(record_folder + stock_level_store_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_level_store(df, title):\n",
    "    df1 = df.sort_values(by=['store_code', 'rotation', 'date_key']).copy()\n",
    "    sl_all_stores = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sl_all_stores['date_key'] = pd.to_datetime(sl_all_stores.date_key, format='%Y%m%d')\n",
    "    fig, axes = plt.subplots(figsize=(14, 3.5), ncols=3)\n",
    "    for i, rotation in enumerate(sl_all_stores.rotation.unique()):\n",
    "        axes[i].set_title(f'Store stock for {title} Rotation {rotation}')\n",
    "        for dm in sl_all_stores.in_dm.unique():\n",
    "            d = sl_all_stores[(sl_all_stores.in_dm == dm) & (sl_all_stores.rotation == rotation)]\n",
    "            axes[i].plot(d.date_key, d.stock_level, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes[i].legend()\n",
    "    fig.autofmt_xdate(); fig.tight_layout()\n",
    "    \n",
    "    df1.to_excel(stock_store_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store, \"all items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_store_writer.save()\n",
    "print(f'Please check file {stock_level_store_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dt(x):\n",
    "    return pd.to_datetime(x, format='%Y%m%d')\n",
    "\n",
    "def process_for_consistency(sim_hist):\n",
    "    ord0 = sim_hist[sim_hist.order_day == sim_hist.run_date]\n",
    "    if ord0.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    chk = ord0[['sub_id', 'item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "                'supplier_name', 'order_day', 'order_qty_box']] \\\n",
    "        .rename(columns={'order_qty_box': 'order_qty (box)'}).copy()\n",
    "    for i in range(2, 8):  # week 2~7\n",
    "        ord1 = sim_hist[(to_dt(sim_hist.order_day) - to_dt(sim_hist.run_date)).dt.days == int(i * 7)]\n",
    "        chk = chk.merge(ord1[['sub_id', 'order_day', 'order_qty_box']]\n",
    "                        .rename(columns={'order_qty_box': f'Week-{i}_before_order_qty'}),\n",
    "                        on=['sub_id', 'order_day'], how='left')\n",
    "\n",
    "    ## aggregate by order week\n",
    "    chk['order_day'] = to_dt(chk.order_day)\n",
    "    chk['order_week'] = (chk.order_day - pd.to_timedelta(chk.order_day.dt.weekday, unit='days')\n",
    "                        ).dt.strftime('%Y%m%d')\n",
    "    chk['Week of year'] = 'W' + chk['order_day'].dt.week.astype(str)\n",
    "    chk = chk.drop(columns=['order_day', 'sub_id'])\n",
    "    chk = chk.groupby(['item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "                       'supplier_name', 'order_week', 'Week of year']).agg(lambda x: x.sum(skipna=False)).reset_index()\n",
    "\n",
    "    for i in range(2, 8):  # calculate consistency error\n",
    "        chk[f'Week-{i}_before_error'] = (\n",
    "            chk[f'Week-{i}_before_order_qty'] -\n",
    "            chk['order_qty (box)']) / (chk['order_qty (box)'] + 1.e-6)\n",
    "        chk[f'Week-{i}_before_run_date'] = (to_dt(chk.order_week) -\n",
    "                                            datetime.timedelta(days=i*7)).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    columns = ['item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "               'supplier_name', 'order_week', 'order_qty (box)', 'Week of year']\n",
    "    for i in range(2, 8):\n",
    "        columns += [f'Week-{i}_before_run_date', f'Week-{i}_before_order_qty', f'Week-{i}_before_error']\n",
    "    return chk[columns]\n",
    "\n",
    "def xavier_method_v2(forecast, actual):\n",
    "    ''' sum(diff) / sum(forecast) '''\n",
    "    return (actual - forecast).abs().sum() / (actual.sum() + 1.e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only trigger on Sunday\n",
    "\n",
    "if datetime.datetime.strptime(date_str, '%Y%m%d').date().weekday() == 6:\n",
    "    order_hist = spark.sql(get_query(\n",
    "    'sql/kpi_consistency_order_hist.sql',\n",
    "    database_name='vartefact', CONSISTENCY_START=CONSISTENCY_START, CONSISTENCY_END=CONSISTENCY_END,\n",
    ")).toPandas()\n",
    "    \n",
    "    _onstock = order_hist[order_hist.rotation.isin(['A', 'B'])].copy()\n",
    "    _onstock['order_qty_box'] = np.ceil(_onstock['order_qty'].astype('f8') / _onstock['pcb'].astype('f8'))\n",
    "    chk_onstock = process_for_consistency(_onstock)\n",
    "\n",
    "    _xdock = order_hist[order_hist.rotation == 'X'].copy()\n",
    "    _xdock['order_qty_box'] = np.ceil(_xdock['order_qty'].astype('f8') / _xdock['pcb'].astype('f8'))\n",
    "    _xdocking = (_xdock.groupby(['order_day', 'run_date', 'sub_id', 'rotation', 'item_id', 'item_code',\n",
    "                                 'supplier_name', 'barcode', 'item_name'])['order_qty_box'].sum().reset_index())\n",
    "    chk_xdocking = process_for_consistency(_xdocking)\n",
    "\n",
    "    if chk_xdocking.shape[0] > 0 and chk_onstock.shape[0] > 0:\n",
    "        ## Merge, keep only latest week, save to excel\n",
    "        chk = pd.concat([chk_onstock, chk_xdocking], axis=0)\n",
    "        chk = chk[chk.order_week == chk.order_week.max()]\n",
    "        chk.sort_values(by='item_code') \\\n",
    "           .to_excel(record_folder + consistency_file,\n",
    "                     sheet_name='template', index=False)\n",
    "        ## Print overall consistency (only print week 2)\n",
    "        flow_a = chk.query('rotation == \"A\"')\n",
    "        flow_b = chk.query('rotation == \"B\"')\n",
    "        flow_x = chk.query('rotation == \"X\"')\n",
    "        print('Consistency (week 2):\\nOverall: {:.2f}\\nFlow A: {:.2f}%\\nFlow B: {:.2f}%\\nFlow X: {:.2f}%'\n",
    "              .format(\n",
    "                  xavier_method_v2(chk['Week-2_before_order_qty'], chk['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_a['Week-2_before_order_qty'], flow_a['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_b['Week-2_before_order_qty'], flow_b['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_x['Week-2_before_order_qty'], flow_x['order_qty (box)'])\n",
    "              ))\n",
    "    else:\n",
    "        print(f'There is not enough order history data between {CONSISTENCY_START} and '\n",
    "              f'{CONSISTENCY_END} to calculate consistency.')\n",
    "else:\n",
    "    print('Today is not Sunday. Will not calculate consistency.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Service level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Service level - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_writer = ExcelWriter(record_folder + service_level_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_dc = read_query_and_fetch(\n",
    "    'sql/kpi_service_level.sql',\n",
    "    database_name='vartefact', date_start=SERVICE_LEVEL_START, date_end=SERVICE_LEVEL_END,\n",
    "    kudu_replace={'lfms.daily_dctrxn': 'lfms_daily_dctrxn', 'lfms.ord': 'lfms_ord'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_service_level_dc(df):\n",
    "    df1 = df.copy()\n",
    "    for col in ['trxn_qty_sum', 'basic_order_qty_sum', 'service_level']:\n",
    "        df1[col] = df1[col].astype('f8')\n",
    "\n",
    "    df1 = df1.rename(columns={'trxn_qty_sum': 'received_qty_sum', \n",
    "                              'basic_order_qty_sum': 'ordered_qty_sum'})\n",
    "    \n",
    "    df1.to_excel(sl_writer, sheet_name='DC service level', index=False)\n",
    "    return df1.groupby('holding_code')[['service_level']].mean().reset_index().style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DC service level:')\n",
    "show_service_level_dc(sl_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Service level - Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_store = read_query_and_fetch(\n",
    "    'sql/kpi_service_level_store.sql',\n",
    "    database_name='vartefact', date_start=SERVICE_LEVEL_START, date_end=SERVICE_LEVEL_END,\n",
    "   kudu_replace={'lfms.daily_shipment':'lfms_daily_shipment'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_service_level_store(df):\n",
    "    df1 = df.copy()\n",
    "    for col in ['order_qty_in_sku_sum', 'delivery_qty_in_sku_sum', 'service_level']:\n",
    "        df1[col] = df1[col].astype('f8')\n",
    "\n",
    "    df1 = df1.rename(columns={'delivery_qty_in_sku_sum': 'store_received_qty_sum', \n",
    "                              'order_qty_in_sku_sum': 'store_ordered_qty_sum'})\n",
    "    \n",
    "    df1.to_excel(sl_writer, sheet_name='Store service level', index=False)\n",
    "    return df1.groupby('holding_code')[['service_level']].mean().reset_index().style.hide_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('store service level:')\n",
    "show_service_level_store(sl_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Out-of-Stock item list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_writer = ExcelWriter(record_folder + oos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_list_dc = read_query_and_fetch(\n",
    "    'sql/kpi_oos_item_list_dc.sql',\n",
    "    # database_name='vartefact', date_start='20190701', date_end='20190730',\n",
    "    database_name='vartefact', oos_check_date=OOS_CHECK_DATE,\n",
    "    kudu_replace={'lfms.daily_dcstock': 'lfms_daily_dcstock'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(oos_item_list_dc)} out of stock DC items found')\n",
    "oos_item_list_dc.to_excel(oos_writer, sheet_name='Out of stock items in DC', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_list_store = read_query_and_fetch(\n",
    "    'sql/kpi_oos_item_list_store.sql',\n",
    "    # database_name='vartefact', date_start='20190701', date_end='20190730',\n",
    "    database_name='vartefact', oos_check_date=OOS_CHECK_DATE,\n",
    "    # kudu_replace={'lfms.daily_dcstock': 'lfms_daily_dcstock'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_store = oos_item_list_store.groupby(['store_code'])['full_item_code'].count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(by=['full_item_code'], ascending = False) \\\n",
    "        .copy()\n",
    "\n",
    "oos_item_store.columns = [\"Store code\", \"Number of items out of stock\"]\n",
    "\n",
    "oos_item_store.to_excel(oos_writer, sheet_name='Out of stock in store', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 stores with most out of stock items\")\n",
    "oos_item_store.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item = oos_item_list_store.groupby(['full_item_code', 'item_id', 'sub_id',\n",
    "                                        'cn_name', 'rotation', 'ds_supplier_code',\n",
    "                                       'store_status','dc_status'])['store_code'].count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(by=['store_code'], ascending = False) \\\n",
    "        .copy() \\\n",
    "\n",
    "oos_item.columns = [\"Full item code\", \"Item ID\", \"Sub ID\", \n",
    "                    \"CN name\", 'Rotation', 'DS Supplier Code',\n",
    "                   \"Store status\",\"DC status\", \"Number out of stock stores\"]\n",
    "\n",
    "oos_item.to_excel(oos_writer, sheet_name='Out of stock items', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 out of stock items\")\n",
    "oos_item.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(oos_item_list_store)} out of stock store items found')\n",
    "oos_item_list_store.to_excel(oos_writer, sheet_name='Out of stock items in store', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_predict_item_store = oos_item_list_store[oos_item_list_store[\"sales_prediction\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_predict_item = no_predict_item_store[['full_item_code', 'item_id', 'sub_id', \n",
    "                                         'cn_name', 'rotation', 'con_holding', 'ds_supplier_code',\n",
    "                                        'store_status','dc_status']] \\\n",
    "                    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Items that does not have sales prediction\")\n",
    "no_predict_item.to_excel(oos_writer, sheet_name='No sales prediction item', index=False)\n",
    "no_predict_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_writer.save()\n",
    "print(f'Please check file {oos_file} for detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Generating monitoring report takes {T1-T0:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
