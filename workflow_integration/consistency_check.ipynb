{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Planning KPI Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, datetime, time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib. dates as mdates\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from impala.dbapi import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default parameters.\n",
      "Consistency:\t20190724\t20190922\n",
      "OOS_CHECK_DATE:\t20190922\n"
     ]
    }
   ],
   "source": [
    "## Get parameters, if not given then fall back to default values\n",
    "\n",
    "tfmt = '%Y%m%d'\n",
    "_end = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime(tfmt)\n",
    "\n",
    "if 'MONITOR_RUN_DATE' in os.environ:\n",
    "    print('Using external parameters.')\n",
    "    _end = os.environ.get('MONITOR_RUN_DATE')\n",
    "else:\n",
    "    print('Using default parameters.')\n",
    "    \n",
    "_start = (datetime.datetime.strptime(_end, '%Y%m%d').date() - datetime.timedelta(days=60)).strftime(tfmt)\n",
    "date_str = _end\n",
    "\n",
    "CONSISTENCY_START, CONSISTENCY_END = _start, _end\n",
    "OOS_CHECK_DATE = _end\n",
    "\n",
    "print('Consistency:', CONSISTENCY_START, CONSISTENCY_END, sep='\\t')\n",
    "print('OOS_CHECK_DATE:', OOS_CHECK_DATE, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_folder = '/data/jupyter/Carrefour-China-Supply-Chain-Forecast/output/monitoring/'\n",
    "\n",
    "consistency_file = f'report_consistency_items_{date_str}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generation time: 2019-09-23 08:46:05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Report generation time:', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), end='\\n\\n')\n",
    "T0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars /data/jupyter/kudu-spark2_2.11-1.8.0.jar pyspark-shell'\n",
    "warehouse_location = os.path.abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Forecast consistency check\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.blacklist.enabled\", False) \\\n",
    "    .config(\"spark.driver.memory\", '6g') \\\n",
    "    .config(\"spark.executor.memory\", '6g') \\\n",
    "    .config(\"spark.num.executors\", '14') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudu_tables = [\n",
    "    'lfms.daily_dctrxn', 'lfms.daily_dcstock', 'lfms.ord', 'lfms.daily_shipment'\n",
    "]\n",
    "\n",
    "for tbl in kudu_tables:\n",
    "    spark.read.format('org.apache.kudu.spark.kudu') \\\n",
    "    .option('kudu.master', \"dtla1apps11:7051,dtla1apps12:7051,dtla1apps13:7051\") \\\n",
    "    .option('kudu.table', f'impala::{tbl}') \\\n",
    "    .load() \\\n",
    "    .registerTempTable('{}'.format(tbl.replace('.', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(sql_path, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "   \n",
    "    query = query.format(**query_params)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query_and_fetch(sql_path, create_table=False, get_query=False, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "    if not create_table:\n",
    "        ## remove lines with `table`\n",
    "        q0 = query\n",
    "        query = '\\n'.join([line for line in q0.split('\\n')\n",
    "                           if ('drop table' not in line.lower())\n",
    "                           and ('create table' not in line.lower())])\n",
    "    query = query.format(**query_params)\n",
    "    if get_query:\n",
    "        return query\n",
    "    return spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_with_impala(sql):\n",
    "    with connect(host='dtla1apps14', port=21050, auth_mechanism='PLAIN', user='CHEXT10211', password='datalake2019',\n",
    "                 database='vartefact') as conn:\n",
    "        curr = conn.cursor()\n",
    "        curr.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dt(x):\n",
    "    return pd.to_datetime(x, format='%Y%m%d')\n",
    "\n",
    "def process_for_consistency(sim_hist):\n",
    "    ord0 = sim_hist[sim_hist.order_day == sim_hist.run_date]\n",
    "    if ord0.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    chk = ord0[['sub_id', 'item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "                'supplier_name', 'order_day', 'order_qty_box']] \\\n",
    "        .rename(columns={'order_qty_box': 'order_qty (box)'}).copy()\n",
    "    for i in range(2, 8):  # week 2~7\n",
    "        ord1 = sim_hist[(to_dt(sim_hist.order_day) - to_dt(sim_hist.run_date)).dt.days == int(i * 7)]\n",
    "        chk = chk.merge(ord1[['sub_id', 'order_day', 'order_qty_box']]\n",
    "                        .rename(columns={'order_qty_box': f'Week-{i}_before_order_qty'}),\n",
    "                        on=['sub_id', 'order_day'], how='left')\n",
    "\n",
    "    ## aggregate by order week\n",
    "    chk['order_day'] = to_dt(chk.order_day)\n",
    "    chk['order_week'] = (chk.order_day - pd.to_timedelta(chk.order_day.dt.weekday, unit='days')\n",
    "                        ).dt.strftime('%Y%m%d')\n",
    "    chk['Week of year'] = 'W' + chk['order_day'].dt.week.astype(str)\n",
    "    chk = chk.drop(columns=['order_day', 'sub_id'])\n",
    "    chk = chk.groupby(['item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "                       'supplier_name', 'order_week', 'Week of year']).agg(lambda x: x.sum(skipna=False)).reset_index()\n",
    "\n",
    "    for i in range(2, 8):  # calculate consistency error\n",
    "        chk[f'Week-{i}_before_error'] = (\n",
    "            chk[f'Week-{i}_before_order_qty'] -\n",
    "            chk['order_qty (box)']) / (chk['order_qty (box)'] + 1.e-6)\n",
    "        chk[f'Week-{i}_before_run_date'] = (to_dt(chk.order_week) -\n",
    "                                            datetime.timedelta(days=i*7)).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    columns = ['item_id', 'item_code', 'item_name', 'barcode', 'rotation',\n",
    "               'supplier_name', 'order_week', 'order_qty (box)', 'Week of year']\n",
    "    for i in range(2, 8):\n",
    "        columns += [f'Week-{i}_before_run_date', f'Week-{i}_before_order_qty', f'Week-{i}_before_error']\n",
    "    return chk[columns]\n",
    "\n",
    "def xavier_method_v2(forecast, actual):\n",
    "    ''' sum(diff) / sum(forecast) '''\n",
    "    return (actual - forecast).abs().sum() / (actual.sum() + 1.e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only trigger on Sunday\n",
    "\n",
    "if datetime.datetime.strptime(date_str, '%Y%m%d').date().weekday() == 6:\n",
    "    order_hist = spark.sql(get_query(\n",
    "    'sql/kpi_consistency_order_hist.sql',\n",
    "    database_name='vartefact', CONSISTENCY_START=CONSISTENCY_START, CONSISTENCY_END=CONSISTENCY_END,\n",
    ")).toPandas()\n",
    "    \n",
    "    _onstock = order_hist[order_hist.rotation.isin(['A', 'B'])].copy()\n",
    "    _onstock['order_qty_box'] = np.ceil(_onstock['order_qty'].astype('f8') / _onstock['pcb'].astype('f8'))\n",
    "    chk_onstock = process_for_consistency(_onstock)\n",
    "\n",
    "    _xdock = order_hist[order_hist.rotation == 'X'].copy()\n",
    "    _xdock['order_qty_box'] = np.ceil(_xdock['order_qty'].astype('f8') / _xdock['pcb'].astype('f8'))\n",
    "    _xdocking = (_xdock.groupby(['order_day', 'run_date', 'sub_id', 'rotation', 'item_id', 'item_code',\n",
    "                                 'supplier_name', 'barcode', 'item_name'])['order_qty_box'].sum().reset_index())\n",
    "    chk_xdocking = process_for_consistency(_xdocking)\n",
    "\n",
    "    if chk_xdocking.shape[0] > 0 and chk_onstock.shape[0] > 0:\n",
    "        ## Merge, keep only latest week, save to excel\n",
    "        chk = pd.concat([chk_onstock, chk_xdocking], axis=0)\n",
    "        chk = chk[chk.order_week == chk.order_week.max()]\n",
    "        chk.sort_values(by='item_code') \\\n",
    "           .to_excel(record_folder + consistency_file,\n",
    "                     sheet_name='template', index=False)\n",
    "        ## Print overall consistency (only print week 2)\n",
    "        flow_a = chk.query('rotation == \"A\"')\n",
    "        flow_b = chk.query('rotation == \"B\"')\n",
    "        flow_x = chk.query('rotation == \"X\"')\n",
    "        print('Consistency (week 2):\\nOverall: {:.2f}\\nFlow A: {:.2f}%\\nFlow B: {:.2f}%\\nFlow X: {:.2f}%'\n",
    "              .format(\n",
    "                  xavier_method_v2(chk['Week-2_before_order_qty'], chk['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_a['Week-2_before_order_qty'], flow_a['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_b['Week-2_before_order_qty'], flow_b['order_qty (box)']),\n",
    "                  xavier_method_v2(flow_x['Week-2_before_order_qty'], flow_x['order_qty (box)'])\n",
    "              ))\n",
    "    else:\n",
    "        print(f'There is not enough order history data between {CONSISTENCY_START} and '\n",
    "              f'{CONSISTENCY_END} to calculate consistency.')\n",
    "else:\n",
    "    print('Today is not Sunday. Will not calculate consistency.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Generating consistency report takes {T1-T0:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
