package carrefour.forecast.util

import carrefour.forecast.config.SimulationTables
import carrefour.forecast.model.EnumFlowType.FlowType
import carrefour.forecast.model.{DateRow, ItemEntity, ModelRun}
import carrefour.forecast.queries.SimulationQueries
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{col, lit, when}

/**
  * Utility used for simulation process
  */
object SimulationUtil {

  /**
    * Insert simulation result to datalake
    * 将模拟运行结果写入datalake
    *
    * @param resDf 模拟运行结果
    * @param modelRun Job run information 脚本运行信息
    * @param sqlc Spark SQLContext
    */
  def insertSimulationResultToDatalake(resDf: Dataset[DateRow], modelRun: ModelRun, sqlc: SQLContext): Unit = {
    val simulationDf = resDf.filter("error_info == ''")

    val simulationView = modelRun.flowType + "_simulation_df"

    simulationDf.createOrReplaceTempView(simulationView)

    val simulationSql =
      s"""
       insert overwrite table ${SimulationTables.simulationResultTable}
         partition(run_date, flow_type)
         select date_key, item_id, sub_id, dept_code, item_code, sub_code,
         con_holding, store_code, supplier_code, rotation,
         order_day, delivery_day, order_qty, order_without_pcb,
         is_order_day, matched_sales_start_date, matched_sales_end_date,
         start_stock, future_stock,
         CASE WHEN minimum_stock_required IS NULL
          THEN average_sales
          ELSE minimum_stock_required
         END as minimum_stock_required,
         dm_delivery, order_delivery,
         predict_sales, day_end_stock_with_predict, actual_sales, day_end_stock_with_actual,
         ittreplentyp, shelf_capacity, ittminunit,
         '${modelRun.runDateStr}' as run_date, '${modelRun.flowType}' as flow_type
         from ${simulationView}
        """

    sqlc.sql(simulationSql)

    sqlc.sql(s"refresh ${SimulationTables.simulationResultTable} ")
  }

  /**
    * Insert orders generated by simulation process to datalake
    * 将模拟运行计算的订单规划写入datalake
    *
    * @param orderDf Orders generated by simulation process 模拟运行计算的订单规划
    * @param modelRun Job run information 脚本运行信息
    * @param sqlc Spark SQLContext
    */
  def insertSimulationOrderToDatalake(orderDf: Dataset[DateRow], modelRun: ModelRun, sqlc: SQLContext): Unit = {

    val finalOrderDf = orderDf.filter("error_info == ''")

    val orderDfView = modelRun.flowType + "_result_df"

    finalOrderDf.createOrReplaceTempView(orderDfView)

    val order_hist_sql =
      s"""
        insert overwrite table ${SimulationTables.simulationOrdersHistTable}
        partition(run_date, flow_type)
         select item_id, sub_id, dept_code, item_code, sub_code,
         con_holding, store_code, supplier_code, rotation,
         order_day, delivery_day,
         CASE WHEN minimum_stock_required IS NULL
          THEN average_sales
          ELSE minimum_stock_required
         END as minimum_stock_required,
         order_qty, order_without_pcb,
         ${modelRun.runDateStr}, '${modelRun.flowType}' as flow_type
         from ${orderDfView}
        """

    sqlc.sql(order_hist_sql)

    sqlc.sql(s"refresh ${SimulationTables.simulationOrdersHistTable} ")
  }

  /**
    * Get on the way order quantity and delivery date from simulation process
    * 获取模拟运行生成的在途订单订货量及其抵达日期
    *
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param isDcFlow Whether it is DC flow 是否为计算DC/货仓订单
    * @param viewName Temp view name used by job run 脚本运行时使用的临时数据库视图名
    * @param spark Spark session
    * @return On the way order quantity and delivery date from simulation process 模拟运行生成的在途订单订货量及其抵达日期
    */
  def getSimulationOnTheWayStockMap(startDateStr: String, endDateStr: String, isDcFlow: Boolean, viewName: String,
                                    spark: SparkSession): Map[ItemEntity, List[Tuple2[String, Double]]] = {
    import spark.implicits._

    val onTheWayStockSql = SimulationQueries.getSimulationOnTheWayStockSql(startDateStr, endDateStr,
      viewName, isDcFlow)

    val onTheWayStockDf = spark.sqlContext.sql(onTheWayStockSql)

    val grouppedDf = onTheWayStockDf.groupByKey(row => ItemEntity(row.getAs[Integer]("item_id"),
      row.getAs[Integer]("sub_id"),
      row.getAs[String]("entity_code"),
      isDcFlow))

    val onTheWayStockMap = grouppedDf.mapGroups((itemEntity, rows) => {
      var onTheWayStockList: List[Tuple2[String, Double]] = List()
      for (row <- rows) {
        onTheWayStockList = (row.getAs[String]("delivery_day"),
          row.getAs[Double]("order_qty")) :: onTheWayStockList
      }
      itemEntity -> onTheWayStockList
    }

    ).collect().toMap

    return onTheWayStockMap
  }

  /**
    * Get future store orders to DC generated by simulation process
    * 获取模拟运行计算的门店向DC/货仓未来订货量
    *
    * @param dateMapDf All combinations of item, store, and dates 全部商品，门店，及日期的组合
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param viewName Temp view name used by job run 脚本运行时使用的临时数据库视图名
    * @param sqlc Spark SQLContext
    * @return Future store orders to DC generated by simulation process 模拟运行计算的门店向DC/货仓未来订货量
    */
  def getSimulationStoreOrderToDc(dateMapDf: DataFrame, startDateStr: String, endDateStr: String, viewName: String,
                                  sqlc: SQLContext): DataFrame = {

    // Get sales prediction
    val storeOrderToDcSql = SimulationQueries.getSimulationStoreOrderToDcSql(startDateStr, endDateStr, viewName)
    var storeOrderToDcDf = sqlc.sql(storeOrderToDcSql)

    storeOrderToDcDf = dateMapDf.join(storeOrderToDcDf, Seq("item_id", "sub_id", "entity_code", "date_key"), "left")
    storeOrderToDcDf = storeOrderToDcDf.withColumn("predict_sales",
      when(col("daily_sales_prediction").isNull, lit(0.0)).otherwise(col("daily_sales_prediction")))
    storeOrderToDcDf = storeOrderToDcDf.drop("daily_sales_prediction")

    return storeOrderToDcDf
  }

  /**
    * Get actual sales
    * 获取真实历史销量
    *
    * @param dateMapDf All combinations of item, store, and dates 全部商品，门店，及日期的组合
    * @param modelRun Job run information 脚本运行信息
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param sqlc Spark SQLContext
    * @return Actual sales 真实历史销量
    */
  def getActualSales(dateMapDf: DataFrame, modelRun: ModelRun, startDateStr: String, endDateStr: String,
                             sqlc: SQLContext): DataFrame = {

    modelRun.flowType match {
      case FlowType.XDocking | FlowType.OnStockStore => {
        SimulationUtil
          .getStoreActualSales(dateMapDf, startDateStr, endDateStr, modelRun.viewName, sqlc)
      }

      case FlowType.DC => {
        SimulationUtil
          .getDCActualSales(dateMapDf, startDateStr, endDateStr, modelRun.viewName, sqlc)
      }
    }

  }

  /**
    * Get store actual sales
    * 获取门店真实历史销量
    *
    * @param dateMapDf All combinations of item, store, and dates 全部商品，门店，及日期的组合
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param viewName Temp view name used by job run 脚本运行时使用的临时数据库视图名
    * @param sqlc Spark SQLContext
    * @return Store actual sales 门店真实历史销量
    */
  def getStoreActualSales(dateMapDf: DataFrame, startDateStr: String, endDateStr: String, viewName: String,
                          sqlc: SQLContext): DataFrame = {

    // Get actual sales
    val actualSalesSql = SimulationQueries.getActualSalesSql(startDateStr, endDateStr, viewName)
    var actualSalesDf = sqlc.sql(actualSalesSql)

    actualSalesDf = dateMapDf.join(actualSalesDf, Seq("item_id", "sub_id", "entity_code", "date_key"), "left")
    actualSalesDf = actualSalesDf.withColumn("actual_sales",
      when(col("daily_sales_sum").isNull, lit(0.0)).otherwise(col("daily_sales_sum")))
    actualSalesDf = actualSalesDf.drop("daily_sales_sum")

    return actualSalesDf
  }

  /**
    * Get store actual orders to DC generated by simulation process
    * 获取模拟运行计算的门店向DC/货仓真实订货量
    *
    * @param dateMapDf All combinations of item, store, and dates 全部商品，门店，及日期的组合
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param viewName Temp view name used by job run 脚本运行时使用的临时数据库视图名
    * @param sqlc Spark SQLContext
    * @return Store actual orders to DC generated by simulation process 模拟运行计算的门店向DC/货仓真实订货量
    */
  def getDCActualSales(dateMapDf: DataFrame, startDateStr: String, endDateStr: String, viewName: String,
                       sqlc: SQLContext): DataFrame = {

    // Get actual sales
    val actualSalesSql = SimulationQueries.getSimulationDcActualSales(startDateStr, endDateStr, viewName)
    var actualSalesDf = sqlc.sql(actualSalesSql)

    actualSalesDf = dateMapDf.join(actualSalesDf, Seq("item_id", "sub_id", "entity_code", "date_key"), "left")
    actualSalesDf = actualSalesDf.withColumn("actual_sales",
      when(col("daily_sales_sum").isNull, lit(0.0)).otherwise(col("daily_sales_sum")))
    actualSalesDf = actualSalesDf.drop("daily_sales_sum")

    return actualSalesDf
  }

  /**
    * Get past generated orders for DC generated by simulation process
    * 获取模拟运行计算的过去生成的DC订单规划
    *
    * @param startDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param endDateStr Start date in yyyyMMdd String format 文本格式的起始日期，为yyyyMMdd格式
    * @param isDcFlow Whether it is DC flow 是否为计算DC/货仓订单
    * @param viewName Temp view name used by job run 脚本运行时使用的临时数据库视图名
    * @param spark Spark SQLContext
    * @return Past generated orders for DC generated by simulation process 模拟运行计算的过去生成的DC订单规划
    */
  def getSimulationDcPastOrdersMap(startDateStr: String, endDateStr: String, isDcFlow: Boolean, viewName: String,
                                   spark: SparkSession): Map[ItemEntity, List[Tuple2[String, Double]]] = {
    import spark.implicits._

    val pastOrdersSql = SimulationQueries.getSimulationDcPastOrdersSql(startDateStr, endDateStr, viewName)
    val pastOrdersDf = spark.sqlContext.sql(pastOrdersSql)

    val grouppedDf = pastOrdersDf.groupByKey(row => ItemEntity(row.getAs[Integer]("item_id"),
      row.getAs[Integer]("sub_id"),
      row.getAs[String]("entity_code"),
      isDcFlow))

    val pastOrdersMap = grouppedDf.mapGroups((itemEntity, rows) => {
      var pastOrdersList: List[Tuple2[String, Double]] = List()
      for (row <- rows) {
        pastOrdersList = (row.getAs[String]("order_day"),
          row.getAs[Double]("order_qty")) :: pastOrdersList
      }
      itemEntity -> pastOrdersList
    }

    ).collect().toMap

    return pastOrdersMap
  }
}
