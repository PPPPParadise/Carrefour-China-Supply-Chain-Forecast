{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Planning KPI Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, datetime, time\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 200)\n",
    "pd.set_option('max_rows', 200)\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from impala.dbapi import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get parameters, if not given then fall back to default values\n",
    "\n",
    "tfmt = '%Y%m%d'\n",
    "_end = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime(tfmt)\n",
    "\n",
    "if 'MONITOR_RUN_DATE' in os.environ:\n",
    "    print('Using external parameters.')\n",
    "    _end = os.environ.get('MONITOR_RUN_DATE')\n",
    "else:\n",
    "    print('Using default parameters.')\n",
    "    \n",
    "_start = (datetime.datetime.strptime(_end, '%Y%m%d').date() - datetime.timedelta(days=60)).strftime(tfmt)\n",
    "date_str = _end\n",
    "\n",
    "DETENTION_START, DETENTION_END = _start, _end\n",
    "SERVICE_LEVEL_START, SERVICE_LEVEL_END = _start, _end\n",
    "STOCK_LEVEL_START, STOCK_LEVEL_END = _start, _end\n",
    "CONSISTENCY_START, CONSISTENCY_END = _start, _end\n",
    "OOS_CHECK_DATE = _end\n",
    "\n",
    "print('Detention:', DETENTION_START, DETENTION_END, sep='\\t')\n",
    "print('Stock level:', STOCK_LEVEL_START, STOCK_LEVEL_END, sep='\\t')\n",
    "print('Consistency:', CONSISTENCY_START, CONSISTENCY_END, sep='\\t')\n",
    "print('Service level:', SERVICE_LEVEL_START, SERVICE_LEVEL_END, sep='\\t')\n",
    "print('OOS_CHECK_DATE:', OOS_CHECK_DATE, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_folder = '/data/jupyter/Carrefour-China-Supply-Chain-Forecast/output/monitoring/'\n",
    "\n",
    "detention_rate_dc_file = f\"report_detention_rate_dc_{date_str}.xlsx\"\n",
    "\n",
    "detention_rate_store_file = f\"report_detention_rate_store_{date_str}.xlsx\"\n",
    "\n",
    "stock_level_dc_file = f\"report_stock_level_dc_{date_str}.xlsx\"\n",
    "\n",
    "stock_level_store_file = f\"report_stock_level_store_{date_str}.xlsx\"\n",
    "\n",
    "service_level_file = f\"report_service_level_{date_str}.xlsx\"\n",
    "\n",
    "consistency_file = f'report_consistency_items_{date_str}.xlsx'\n",
    "\n",
    "oos_file = f'report_oos_item_list_{date_str}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Report generation time:', datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), end='\\n\\n')\n",
    "T0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars /data/jupyter/kudu-spark2_2.11-1.8.0.jar pyspark-shell'\n",
    "warehouse_location = os.path.abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Forecast monitoring process\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.blacklist.enabled\", False) \\\n",
    "    .config(\"spark.driver.memory\", '6g') \\\n",
    "    .config(\"spark.executor.memory\", '6g') \\\n",
    "    .config(\"spark.num.executors\", '14') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kudu_tables = [\n",
    "    'lfms.daily_dctrxn', 'lfms.daily_dcstock', 'lfms.ord', 'lfms.daily_shipment'\n",
    "]\n",
    "\n",
    "for tbl in kudu_tables:\n",
    "    spark.read.format('org.apache.kudu.spark.kudu') \\\n",
    "    .option('kudu.master', \"dtla1apps11:7051,dtla1apps12:7051,dtla1apps13:7051\") \\\n",
    "    .option('kudu.table', f'impala::{tbl}') \\\n",
    "    .load() \\\n",
    "    .registerTempTable('{}'.format(tbl.replace('.', '_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(sql_path, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "   \n",
    "    query = query.format(**query_params)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query_and_fetch(sql_path, create_table=False, get_query=False, kudu_replace=None, **query_params):\n",
    "    with open(sql_path, 'r') as f:\n",
    "        query = f.read()\n",
    "    if kudu_replace is not None:\n",
    "        for k, v in kudu_replace.items():\n",
    "            query = query.replace(k, v)\n",
    "    if not create_table:\n",
    "        ## remove lines with `table`\n",
    "        q0 = query\n",
    "        query = '\\n'.join([line for line in q0.split('\\n')\n",
    "                           if ('drop table' not in line.lower())\n",
    "                           and ('create table' not in line.lower())])\n",
    "    query = query.format(**query_params)\n",
    "    if get_query:\n",
    "        return query\n",
    "    return spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_with_impala(sql):\n",
    "    with connect(host='dtla1apps14', port=21050, auth_mechanism='PLAIN', user='CHEXT10211', password='datalake2019',\n",
    "                 database='vartefact') as conn:\n",
    "        curr = conn.cursor()\n",
    "        curr.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_stock_sql = get_query(\n",
    "    'sql/record_dc_stock.sql',\n",
    "    database_name='vartefact', date_start=DETENTION_START, date_end=DETENTION_END\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql_with_impala(dc_stock_sql.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_stock_sql = get_query(\n",
    "    'sql/record_store_stock.sql',\n",
    "    database_name='vartefact', date_start=DETENTION_START, date_end=DETENTION_END\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sql_with_impala(store_stock_sql.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Detention rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Detention rate - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_dc.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_supplier = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_dc_supplier.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_writer = ExcelWriter(record_folder + detention_rate_dc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_dc(df, title):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    flow_A = df1[df1.rotation == 'A']\n",
    "    flow_B = df1[df1.rotation == 'B']\n",
    "    ax.plot(flow_A.date_dt, flow_A.detention_rate, label='Flow A')\n",
    "    ax.plot(flow_B.date_dt, flow_B.detention_rate, label='Flow B')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'DC detention rate for {title}')\n",
    "    ax.axvline(start_day, ls=\"--\")\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    print(f'Latest detention rate for {title}:')\n",
    "    display(df[['date_key', 'rotation', 'detention_rate']].tail(9).style.hide_index())\n",
    "    \n",
    "    df1.to_excel(detention_dc_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc, 'all items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_dc(detention_dc_supplier[detention_dc_supplier['holding_code'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_dc_writer.save()\n",
    "print(f'Please check file {detention_rate_dc_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Detention rate - Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_all_store = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_all_store.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store_supplier = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_store_supplier.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store = spark.sql(get_query(\n",
    "    'sql/kpi_detention_rate_store.sql',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store_writer = ExcelWriter(record_folder + detention_rate_store_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_all_store(df, title):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    flow_A = df1[df1.rotation == 'A']\n",
    "    flow_B = df1[df1.rotation == 'B']\n",
    "    flow_X = df1[df1.rotation == 'X']\n",
    "    \n",
    "    ax.plot(flow_A.date_dt, flow_A.detention_rate, label='Flow A')\n",
    "    ax.plot(flow_B.date_dt, flow_B.detention_rate, label='Flow B')\n",
    "    ax.plot(flow_X.date_dt, flow_X.detention_rate, label='Flow X')\n",
    "    ax.legend()\n",
    "    ax.axvline(start_day, ls=\"--\")\n",
    "    ax.set_title(f'{title} store detention rate by day')\n",
    "    fig.autofmt_xdate()\n",
    "    print(f'Latest detention rate:')\n",
    "\n",
    "    display(df[['date_key', 'rotation', 'detention_rate']].tail(9).style.hide_index())\n",
    "    \n",
    "    flow_A.to_excel(detention_store_writer, sheet_name=f\"{title} Flow A\", index=False)\n",
    "    flow_B.to_excel(detention_store_writer, sheet_name=f\"{title} Flow B\", index=False)\n",
    "    flow_X.to_excel(detention_store_writer, sheet_name=f\"{title} Flow X\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detention_store(df):\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key', 'store_code']).copy()\n",
    "    df1['date_dt'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "\n",
    "    dr = df1.groupby(['store_code', 'rotation'])['detention_rate'].mean().reset_index()\n",
    "    a = dr.loc[dr.rotation == 'A', 'detention_rate'].sort_values().values\n",
    "    b = dr.loc[dr.rotation == 'B', 'detention_rate'].sort_values().values\n",
    "    x = dr.loc[dr.rotation == 'X', 'detention_rate'].sort_values().values\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(16, 4), ncols=3)\n",
    "\n",
    "    axes[0].plot(a, 'o', ms=6)\n",
    "    axes[1].plot(b, 'o', ms=6)\n",
    "    axes[2].plot(x, 'o', ms=6)\n",
    "    \n",
    "    axes[0].set_title('Store detention rate by store: Flow A')\n",
    "    axes[1].set_title('Store detention rate by store: Flow B')\n",
    "    axes[2].set_title('Store detention rate by store: Flow X')\n",
    "    \n",
    "    axes[0].set_ylim(min(0.75, a.min()), 1)\n",
    "    axes[1].set_ylim(min(0.75, b.min()), 1)\n",
    "    axes[2].set_ylim(min(0.75, x.min()), 1)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    df1[df1.rotation == 'A'].to_excel(detention_store_writer, sheet_name=\"By store flow A\", index=False)\n",
    "    df1[df1.rotation == 'B'].to_excel(detention_store_writer, sheet_name=\"By store flow B\", index=False)\n",
    "    df1[df1.rotation == 'X'].to_excel(detention_store_writer, sheet_name=\"By store flow X\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_all_store(detention_all_store, \"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_all_store(detention_store_supplier[detention_store_supplier['con_holding'] == '002'], \"Nestle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_all_store(detention_store_supplier[detention_store_supplier['con_holding'] == '693'], \"P&G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_all_store(detention_store_supplier[detention_store_supplier['con_holding'] == '700'], \"Unilever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detention_store(detention_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detention_store_writer.save()\n",
    "print(f'Please check file {detention_rate_store_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Stock level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Stock level - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_level_dc = spark.sql(get_query(\n",
    "    'sql/kpi_stock_level_dc.sql',\n",
    "    # database_name='vartefact', date_start='20190630', date_end='20190730',\n",
    "    database='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dc_writer = ExcelWriter(record_folder + stock_level_dc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_level_dc(df, title):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "    df1 = df.sort_values(by=['rotation', 'date_key']).copy()\n",
    "    sl = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sl['date_key'] = pd.to_datetime(sl.date_key, format='%Y%m%d')\n",
    "    \n",
    "    sla = df1.groupby(['rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sla['date_key'] = pd.to_datetime(sla.date_key, format='%Y%m%d')\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "    for i, rotation in enumerate(['A', 'B']):\n",
    "        axes[i].set_title(f'DC stock level {title} Rotation {rotation}')\n",
    "        axes[i].axvline(start_day, ls=\"--\")\n",
    "        axes[i].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "        for dm in sl.in_dm.unique():\n",
    "            d = sl[(sl.in_dm == dm) & (sl.rotation == rotation)]\n",
    "            axes[i].plot(d.date_key, d.stock_level, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes[i].legend()\n",
    "            \n",
    "        d = sla[(sla.rotation == rotation)]\n",
    "        axes[i].plot(d.date_key, d.stock_level, label=\"All\")\n",
    "        axes[i].legend()\n",
    "        \n",
    "        \n",
    "    fig.autofmt_xdate(); fig.tight_layout()\n",
    "    \n",
    "    sl_value = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_value'].sum().reset_index()\n",
    "    sl_value['date_key'] = pd.to_datetime(sl_value.date_key, format='%Y%m%d')\n",
    "    \n",
    "    sla_value = df1.groupby(['rotation', 'date_key'])['stock_value'].sum().reset_index()\n",
    "    sla_value['date_key'] = pd.to_datetime(sla_value.date_key, format='%Y%m%d')\n",
    "    \n",
    "    fig_value, axes_value = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "    for i, rotation in enumerate(['A', 'B']):\n",
    "        axes_value[i].set_title(f'DC stock value {title} Rotation {rotation}')\n",
    "        axes_value[i].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "        axes_value[i].axvline(start_day, ls=\"--\")\n",
    "        for dm in sl_value.in_dm.unique():\n",
    "            d = sl_value[(sl_value.in_dm == dm) & (sl_value.rotation == rotation)]\n",
    "            axes_value[i].plot(d.date_key, d.stock_value, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes_value[i].legend()\n",
    "            \n",
    "        d = sla_value[(sla_value.rotation == rotation)]\n",
    "        axes_value[i].plot(d.date_key, d.stock_value, label=\"All\")\n",
    "        axes_value[i].legend()\n",
    "    fig_value.autofmt_xdate(); fig_value.tight_layout()\n",
    "\n",
    "    \n",
    "    df1.to_excel(stock_dc_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc, 'all items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_dc(stock_level_dc[stock_level_dc['holding_code'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dc_writer.save()\n",
    "print(f'Please check file {stock_level_dc_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Stock level - store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_level_store = spark.sql(get_query(\n",
    "    'sql/kpi_stock_level_store.sql',\n",
    "    # database_name='vartefact', date_start='20190630', date_end='20190730',\n",
    "    database_name='vartefact', run_date=STOCK_LEVEL_END\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_store_writer = ExcelWriter(record_folder + stock_level_store_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_level_store(df, title):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "    df1 = df.sort_values(by=['store_code', 'rotation', 'date_key']).copy()\n",
    "    \n",
    "    sl_all_stores = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sl_all_stores['date_key'] = pd.to_datetime(sl_all_stores.date_key, format='%Y%m%d')\n",
    "    \n",
    "    sla_all_stores = df1.groupby(['rotation', 'date_key'])['stock_level'].sum().reset_index()\n",
    "    sla_all_stores['date_key'] = pd.to_datetime(sla_all_stores.date_key, format='%Y%m%d')\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(14, 3.5), ncols=3)\n",
    "    for i, rotation in enumerate(sl_all_stores.rotation.unique()):\n",
    "        axes[i].set_title(f'Store stock for {title} Rotation {rotation}')\n",
    "        axes[i].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "        axes[i].axvline(start_day, ls=\"--\")\n",
    "        for dm in sl_all_stores.in_dm.unique():\n",
    "            d = sl_all_stores[(sl_all_stores.in_dm == dm) & (sl_all_stores.rotation == rotation)]\n",
    "            axes[i].plot(d.date_key, d.stock_level, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes[i].legend()\n",
    "            \n",
    "        d = sla_all_stores[sla_all_stores.rotation == rotation]\n",
    "        axes[i].plot(d.date_key, d.stock_level, label=\"All\")\n",
    "        \n",
    "        axes[i].legend()\n",
    "    fig.autofmt_xdate(); fig.tight_layout()\n",
    "    \n",
    "    sl_value = df1.groupby(['in_dm', 'rotation', 'date_key'])['stock_value'].sum().reset_index()\n",
    "    sl_value['date_key'] = pd.to_datetime(sl_value.date_key, format='%Y%m%d')\n",
    "    \n",
    "    sla_value = df1.groupby(['rotation', 'date_key'])['stock_value'].sum().reset_index()\n",
    "    sla_value['date_key'] = pd.to_datetime(sla_value.date_key, format='%Y%m%d')\n",
    "    \n",
    "    fig_value, axes_value = plt.subplots(figsize=(14, 3.5), ncols=3)\n",
    "    for i, rotation in enumerate(sl_all_stores.rotation.unique()):\n",
    "        axes_value[i].set_title(f'Store stock value {title} Rotation {rotation}')\n",
    "        axes_value[i].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "        axes_value[i].axvline(start_day, ls=\"--\")\n",
    "        for dm in sl_value.in_dm.unique():\n",
    "            d = sl_value[(sl_value.in_dm == dm) & (sl_value.rotation == rotation)]\n",
    "            axes_value[i].plot(d.date_key, d.stock_value, label=\"DM\" if dm else \"Non-DM\")\n",
    "            axes_value[i].legend()\n",
    "            \n",
    "        d = sla_value[sla_value.rotation == rotation]\n",
    "        axes_value[i].plot(d.date_key, d.stock_value, label=\"All\")\n",
    "        axes_value[i].legend()\n",
    "            \n",
    "    fig_value.autofmt_xdate(); fig_value.tight_layout()\n",
    "    \n",
    "    df1.to_excel(stock_store_writer, sheet_name=title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store, \"all items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '002'], 'Nestle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '693'], 'P&G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stock_level_store(stock_level_store[stock_level_store['con_holding'] == '700'], 'Unilever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_store_writer.save()\n",
    "print(f'Please check file {stock_level_store_file} for detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Service level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Service level - DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_writer = ExcelWriter(record_folder + service_level_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_dc = read_query_and_fetch(\n",
    "    'sql/kpi_service_level.sql',\n",
    "    database_name='vartefact', date_start=SERVICE_LEVEL_START, date_end=SERVICE_LEVEL_END,\n",
    "    kudu_replace={'lfms.daily_dctrxn': 'lfms_daily_dctrxn', 'lfms.ord': 'lfms_ord'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_service_level_dc(df):\n",
    "    df1 = df.copy()\n",
    "    for col in ['trxn_qty_sum', 'basic_order_qty_sum', 'service_level']:\n",
    "        df1[col] = df1[col].astype('f8')\n",
    "\n",
    "    df1 = df1.rename(columns={'trxn_qty_sum': 'received_qty_sum', \n",
    "                              'basic_order_qty_sum': 'ordered_qty_sum'})\n",
    "    \n",
    "    df1.to_excel(sl_writer, sheet_name='DC service level', index=False)\n",
    "    return df1.groupby('holding_code')[['service_level']].mean().reset_index().style.hide_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_service_level_dc(df):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "        \n",
    "    df1 = df.sort_values(by=['holding_code', 'date_key']).copy()\n",
    "    df1['service_level'] = df1['service_level'].astype('f8')\n",
    "    df1['date_key'] = pd.to_datetime(df1.date_key, format='%Y%m%d')\n",
    "    \n",
    "    df1 = df1.groupby(['holding_code', 'date_key'])['service_level'].mean().reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    sp_002 = df1[df1.holding_code == '002']\n",
    "    sp_693 = df1[df1.holding_code == '693']\n",
    "    sp_700 = df1[df1.holding_code == '700']\n",
    "    ax.plot(sp_002.date_key, sp_002.service_level, label='Nestle')\n",
    "    ax.plot(sp_693.date_key, sp_693.service_level, label='P&G')\n",
    "    ax.plot(sp_700.date_key, sp_700.service_level, label='Unilever')\n",
    "    ax.axvline(start_day, ls=\"--\")\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_title(f'DC service level')\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_service_level_dc(sl_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average DC service level from {SERVICE_LEVEL_START} to {SERVICE_LEVEL_END}:')\n",
    "show_service_level_dc(sl_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Service level - Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_store = read_query_and_fetch(\n",
    "    'sql/kpi_service_level_store.sql',\n",
    "    database_name='vartefact', date_start=SERVICE_LEVEL_START, date_end=SERVICE_LEVEL_END,\n",
    "   kudu_replace={'lfms.daily_shipment':'lfms_daily_shipment'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_service_level_store(df):\n",
    "    df1 = df.copy()\n",
    "    for col in ['order_qty_in_sku_sum', 'delivery_qty_in_sku_sum', 'service_level']:\n",
    "        df1[col] = df1[col].astype('f8')\n",
    "\n",
    "    df1 = df1.rename(columns={'delivery_qty_in_sku_sum': 'store_received_qty_sum', \n",
    "                              'order_qty_in_sku_sum': 'store_ordered_qty_sum'})\n",
    "    \n",
    "    df1.to_excel(sl_writer, sheet_name='Store service level', index=False)\n",
    "    return df1.groupby(['holding_code','rotation', 'piece_picking'])[['service_level']] \\\n",
    "            .mean().reset_index().style.hide_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_service_level_store(df):\n",
    "    start_day = pd.to_datetime('20190909', format='%Y%m%d')\n",
    "    \n",
    "    df1 = df.sort_values(by=['rotation', 'piece_picking', 'order_date']).copy()\n",
    "    df1['service_level'] = df1['service_level'].astype('f8')\n",
    "    \n",
    "    sl = df1.groupby(['rotation', 'piece_picking', 'order_date'])['service_level'].mean().reset_index()\n",
    "    sl['order_date'] = pd.to_datetime(sl.order_date, format='%Y%m%d')\n",
    "    fig, axes = plt.subplots(figsize=(12, 4), ncols=3)\n",
    "    for i, rotation in enumerate(['A', 'B', 'X']):\n",
    "        axes[i].set_title(f'Store service level Rotation {rotation}')\n",
    "        axes[i].axvline(start_day, ls=\"--\")\n",
    "        for psp in sl.piece_picking.unique():\n",
    "            d = sl[(sl.piece_picking == psp) & (sl.rotation == rotation)]\n",
    "            axes[i].plot(d.order_date, d.service_level, label=\"piece picking\" if psp == \"Y\" else \"regular\")\n",
    "            axes[i].legend()\n",
    "    fig.autofmt_xdate(); fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_service_level_store(sl_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average store service level from {SERVICE_LEVEL_START} to {SERVICE_LEVEL_END}:')\n",
    "show_service_level_store(sl_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Out-of-Stock item list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_writer = ExcelWriter(record_folder + oos_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_list_dc = read_query_and_fetch(\n",
    "    'sql/kpi_oos_item_list_dc.sql',\n",
    "    # database_name='vartefact', date_start='20190701', date_end='20190730',\n",
    "    database_name='vartefact', oos_check_date=OOS_CHECK_DATE,\n",
    "    kudu_replace={'lfms.daily_dcstock': 'lfms_daily_dcstock'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(oos_item_list_dc)} out of stock DC items found')\n",
    "oos_item_list_dc.to_excel(oos_writer, sheet_name='Out of stock items in DC', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_list_store = read_query_and_fetch(\n",
    "    'sql/kpi_oos_item_list_store.sql',\n",
    "    # database_name='vartefact', date_start='20190701', date_end='20190730',\n",
    "    database_name='vartefact', oos_check_date=OOS_CHECK_DATE,\n",
    "    # kudu_replace={'lfms.daily_dcstock': 'lfms_daily_dcstock'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item_store = oos_item_list_store.groupby(['store_code'])['full_item_code'].count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(by=['full_item_code'], ascending = False) \\\n",
    "        .copy()\n",
    "\n",
    "oos_item_store.columns = [\"Store code\", \"Number of items out of stock\"]\n",
    "\n",
    "oos_item_store.to_excel(oos_writer, sheet_name='Out of stock in store', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 stores with most out of stock items\")\n",
    "oos_item_store.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_item = oos_item_list_store.groupby(['full_item_code', 'item_id', 'sub_id',\n",
    "                                        'cn_name', 'rotation', 'ds_supplier_code',\n",
    "                                       'store_status','dc_status',\n",
    "                                       'item_stop_start_date', 'item_stop_end_date'])['store_code'].count() \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(by=['store_code'], ascending = False) \\\n",
    "        .copy() \\\n",
    "\n",
    "oos_item.columns = [\"Full item code\", \"Item ID\", \"Sub ID\", \n",
    "                    \"CN name\", 'Rotation', 'DS Supplier Code',\n",
    "                    \"Store status\",\"DC status\", \n",
    "                    \"Stop start date\",\"Stop end date\",\n",
    "                    \"Number out of stock stores\"]\n",
    "\n",
    "oos_item.to_excel(oos_writer, sheet_name='Out of stock items', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 out of stock items\")\n",
    "oos_item.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(oos_item_list_store)} out of stock store items found')\n",
    "oos_item_list_store.to_excel(oos_writer, sheet_name='Out of stock items in store', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_predict_item_store = oos_item_list_store[oos_item_list_store[\"sales_prediction\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_predict_item = no_predict_item_store[['full_item_code', 'item_id', 'sub_id', \n",
    "                                         'cn_name', 'rotation', 'con_holding', 'ds_supplier_code',\n",
    "                                        'store_status','dc_status',\n",
    "                                        'item_stop_start_date', 'item_stop_end_date']] \\\n",
    "                    .drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Items that does not have sales prediction\")\n",
    "no_predict_item.to_excel(oos_writer, sheet_name='No sales prediction item', index=False)\n",
    "no_predict_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_writer.save()\n",
    "print(f'Please check file {oos_file} for detail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Generating monitoring report takes {T1-T0:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
